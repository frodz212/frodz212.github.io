---
title: "Housing Data"
author: "Felipe Rodriguez"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Explain any transformations or modifications you made to the dataset
```{r}
library(readxl)
library(dplyr)
library(purrr)
library(lm.beta)
library(ggplot2)
```

```{r}
setwd('/Users/feliperodriguez/OneDrive - Bellevue University/Github/dsc520/')

housing <- read_excel('/Users/feliperodriguez/OneDrive - Bellevue University/Github/dsc520/data/week-7-housing.xlsx')
```

## Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.

```{r}
head(housing)
```
```{r}
sale_price <- housing$`Sale Price`
sqft_lot <- housing$sq_ft_lot
sale_price_lm <- lm(`Sale Price` ~ year_built + sq_ft_lot, data=housing)
price_predict_df <- data.frame(`Sale Price` = predict(sale_price_lm, housing), year_built=housing$year_built, sq_ft_lot=housing$sq_ft_lot)
```

I selected year built and square feet lot. I felt that these predictors would influence the outcome os sale price more than others would. 

## Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r}
summary_sale_price_lm <-summary(sale_price_lm)
summary_sale_price_lm
```
The R Squared value is .08259 and Adjusted R Squared is 0.08245. Our R squared values are low for this model, which  means that we are seeing a weak relationship within the variables selected. 

## Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?

```{r}
lm.beta(sale_price_lm)
```
The standardized beta compares the strength of the variable in relation to the dependent variable. For these two fields, year built has more of an effect on sale price than square foot lot. 

## Calculate the confidence intervals for the parameters in your model and explain what the results indicate.
```{r}
t.test(housing$year_built)
```
```{r}
t.test(housing$sq_ft_lot)
```
The 95 % confidence intervals for the parameter year built is 1992 and 1993 and for square feet lot it is 21244 and 23212. This tells us that 95 percent of values will fall between those parameters. 


## Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.

```{r}
priceAnova <- aov(`Sale Price` ~ year_built + sq_ft_lot, data=housing)
priceAnova
```

## Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.

```{r}
casewise_diagnostics <- housing %>% mutate(z_score_price = ((`Sale Price` - mean(`Sale Price`))/sd(`Sale Price`)))
casewise_diagnostics <- arrange(casewise_diagnostics, desc(z_score_price))
casewise_diagnostics <- select(casewise_diagnostics, `Sale Price`, z_score_price)
head(casewise_diagnostics)
```

## Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.
```{r}
r <- rstandard(sale_price_lm, res = +- 2)
residuals_housing <- housing$`Sale Price` - price_predict_df$Sale.Price
```

## Use the appropriate function to show the sum of large residuals.
```{r}
sse <- sum(residuals_housing^2)
```

## Which specific variables have large residuals (only cases that evaluate as TRUE)?
```{r}
housing$residuals <- residuals(sale_price_lm)
```


## Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.
```{r}
hats <- as.data.frame(hatvalues(sale_price_lm))
head(hats)
cookd <- as.data.frame(cooks.distance(sale_price_lm))
head(cookd)
covariance <- vcov(sale_price_lm)
head(covariance)
```

## Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.

```{r}
chi_squared <- chisq.test(housing$`Sale Price`, housing$year_built)
chi_squared
```

## Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.
```{r}
ggplot(data = housing, aes(y = sq_ft_lot, x = housing$`Sale Price`)) +
  geom_point(color='blue') +
  geom_line(color='red',data = price_predict_df, aes(y= sq_ft_lot, x= price_predict_df$Sale.Price))
```

## Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.

```{r}
plot(residuals_housing)
```

```{r}
hist(residuals_housing)
```


## Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?

I believe this model is unbiased. The plots show that the residuals lie in a central locations, and if we have random samples, those random samples will most likley fall into the model. 
